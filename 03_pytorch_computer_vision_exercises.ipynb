{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vex99np2wFVt"
      },
      "source": [
        "# 03. PyTorch Computer Vision Exercises\n",
        "\n",
        "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
        "\n",
        "They're a bunch of fun.\n",
        "\n",
        "You're going to get to write plenty of code!\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/). \n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA). \n",
        "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaeYzOTLwWh2",
        "outputId": "17dd5453-9639-4b01-aa18-7ddbfd5c3253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Apr 16 03:23:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DNwZLMbCzJLk",
        "outputId": "9c150c50-a092-4f34-9d33-b45247fb080d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.0+cu121\n"
          ]
        }
      ],
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "device=\"hello\"\n",
        "if(torch.cuda.is_available()):\n",
        "    device='cuda'\n",
        "else:\n",
        "    device='cpu'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSFX7tc1w-en"
      },
      "source": [
        "## 1. What are 3 areas in industry where computer vision is currently being used?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VyWRkvWGbCXj"
      },
      "outputs": [],
      "source": [
        "#medical line\n",
        "#driving\n",
        "#teaching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBK-WI6YxDYa"
      },
      "source": [
        "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "d1rxD6GObCqh"
      },
      "outputs": [],
      "source": [
        "#Learning the pattern too well and it cannot account for slight variations of patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeYFEqw8xK26"
      },
      "source": [
        "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each. \n",
        "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ocvOdWKcbEKr"
      },
      "outputs": [],
      "source": [
        "#Regularization- dropping neural networks so remaining neurons learn more robust patterns\n",
        "#different model- as this model might be using too much layers for the problem thus it is learning the data too well\n",
        "# reduce noise in data- might be learning the data too well, thus removing the noise might be beneficial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKdEEFEqxM-8"
      },
      "source": [
        "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqZaJIRMbFtS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvf-3pODxXYI"
      },
      "source": [
        "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SHjeuN81bHza"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset MNIST\n",
              "     Number of datapoints: 60000\n",
              "     Root location: data\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: ToTensor(),\n",
              " Dataset MNIST\n",
              "     Number of datapoints: 10000\n",
              "     Root location: data\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: ToTensor())"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision import datasets\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "train_data=datasets.MNIST(root=\"data\",train=True,download=True,transform=transforms.ToTensor())\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "train_data,test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxZW-uAbxe_F"
      },
      "source": [
        "## 6. Visualize at least 5 different samples of the MNIST training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QVFsYi1PbItE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n",
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n",
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n",
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n",
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(5):\n",
        "    img=train_data[i][0]\n",
        "    print(img.shape)\n",
        "    img_squeeze=img.squeeze()\n",
        "    print(img_squeeze.shape)\n",
        "    label=train_data[i][1]\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(img_squeeze, cmap=\"gray\")\n",
        "    plt.title(label)\n",
        "    plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPDzW0wxhi3"
      },
      "source": [
        "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ALA6MPcFbJXQ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE=32\n",
        "\n",
        "train_dataloader=DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "test_dataloader=DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCVfXk5xjYS"
      },
      "source": [
        "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5IKNF22XbKYS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MNIST_model(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "class MNIST_model(torch.nn.Module):\n",
        "    def __init__(self,input_shape,hidden_units,output_shape):\n",
        "        super().__init__()\n",
        "        self.conv_block_1=nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,out_channels=hidden_units,kernel_size=3,stride=1,padding=1),  \n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=hidden_units,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=hidden_units,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2)                   \n",
        "    )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from? \n",
        "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "            nn.Linear(in_features=hidden_units*7*7, \n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.conv_block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "models=MNIST_model(input_shape=1,hidden_units=10,output_shape=10).to('cuda')\n",
        "models\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_3zUr7xlhy"
      },
      "source": [
        "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSo6vVWFbNLD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1CsHhPpxp1w"
      },
      "source": [
        "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YGgZvSobNxu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQwzqlBWxrpG"
      },
      "source": [
        "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj6bDhoWxt2y"
      },
      "source": [
        "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leCTsqtSbR5P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHS20cNTxwSi"
      },
      "source": [
        "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset. \n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been. \n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error? \n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78a8LjtdbSZj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMUsDcN/+FAm9Pf7Ifqs6AZ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "03_pytorch_computer_vision_exercises.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
